{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDES NEURONALES PROFUNDAS\n",
    "Utilice el conjunto de datos Fashion-MNIST para construir un clasificador convolucional de imágenes de productos. Para la construcción del modelo utilice los dos esquemas que se describen a continuación y compare los resultados:\n",
    "\n",
    "1. Entrenar de un Autocodificador convolucional multicapa.\n",
    "2. Extraer la reducción de la dimensionalidad que el Autocodificador construye en el entrenamiento.\n",
    "\n",
    "Para los puntos 2) compruebe, y muestre con ejemplos, que las imágenes están bien reconstruidas.\n",
    "\n",
    "## OBJETIVO\n",
    "Aplicar el proceso de aprendizaje a partir de datos para resolver problemas de clasificación utilizando redes neuronales convolucionales profundas sobre la herramienta Keras.\n",
    "\n",
    "## DATOS\n",
    "Incluidos en Keras.\n",
    "También, existe otra fuente equivalente que se consigue en el siguiente URL https://www.kaggle.com/zalando-research/fashionmnist donde hay un resumen de estos datos en el archivo CVS y XLSX.\n",
    "\n",
    "la clasificación para el aprendizaje supervisado es:\n",
    "\n",
    "    Label \tClass\n",
    "    0 \t \tT-shirt/top\n",
    "    1 \t \tTrouser\n",
    "    2 \t \tPullover\n",
    "    3 \t \tDress\n",
    "    4 \t \tCoat\n",
    "    5 \t \tSandal\n",
    "    6 \t \tShirt\n",
    "    7 \t \tSneaker\n",
    "    8 \t \tBag\n",
    "    9 \t \tAnkle boot\n",
    "\n",
    "**Importante: Lea los comentarios y apuntes del Notebook para tener claridad de los pasos.**\n",
    "### Consideraciones\n",
    "- Utilice sólo los conjuntos de datos indicado.\n",
    "- El frameworks a utilizar es TensorFlow, Keras con Jupyter Notebbooks.\n",
    "\n",
    "### Enlaces de interés\n",
    "- documentación Keras, URL: https://keras.io/models/sequential/\n",
    "- documentación TensorFlow, URL: https://www.tensorflow.org/versions\n",
    "- Tutorial CNN basico, URL: https://www.kaggle.com/nhlr21/deep-keras-cnn-tutorial/notebook\n"
   ]
  },
  {
   "source": [
    "## Importando Librerias"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando dependencias de trabajo\n",
    "# importando librerias basicas\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "\n",
    "# importando modulos de analisis de datos, ML y graficas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "\n",
    "# importando dependencias para tensorflow\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# importando para sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# importando para keras\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "source": [
    "## Funciones Utiles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que recibe una lista numpy y recupera la forma de cada elemento, devuelve una lista con formas\n",
    "def get_shape(data):\n",
    "    # respuesta de la funcion\n",
    "    ans = list()\n",
    "\n",
    "    for d in data:\n",
    "        sp = d.shape\n",
    "        ans.append(sp)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que transforma el entero de la clase a la palabra de la etiqueta, devuelve una lista de etiquetas\n",
    "def class2label(data, labels):\n",
    "    # respuesta de la funcion\n",
    "    ans = list()\n",
    "\n",
    "    for d in data:\n",
    "        d = int(d)\n",
    "        l = str(labels[d])\n",
    "        ans.append(l)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que estandariza los datos en numpy de acuerdo a un valor min & max, devuelve un arreglo np flotante\n",
    "def std_data(data, minv, maxv):\n",
    "    rangev = maxv - minv\n",
    "    ans = data.astype(\"float32\")/float(rangev)\n",
    "    # ans = pd.Series(ans)\n",
    "    # respuesta de la funcion\n",
    "    return ans"
   ]
  },
  {
   "source": [
    "## Cargar y Preparar los Datos\n",
    "\n",
    "Los pasos de esta seccion son:\n",
    "\n",
    "1. Leer los datos desde MNIST.\n",
    "2. Formatear los datos para que los acepte el DataFrame de Pandas.\n",
    "2. Crear el DataFrame de Pandas con un esquema propio.\n",
    "2. Formatear los datos MNIST para pobrar el DataFrame de pandas.\n",
    "3. Revisar que todo este como deberia estar."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de nombres de las clasificaciones\n",
    "label_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se carga el archivo de datos de trabajo por medio de Keras\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombres de columnas para el dataframe de pandas\n",
    "col_names = [\"img_data\", \"img_shape\", \"class\", \"label\", \"std_img_data\", \"cat_labels\"]#, \"ReshapeData\", \"Label\", \"Class\", \"DataSize\", \"ReshapeSize\", \"ResKeras\", \"ScoreKeras\"]\n",
    "# creando dataframe con columnas\n",
    "fashion_df = pd.DataFrame(columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrando datos de mnist\n",
    "img_data = np.concatenate((x_train, x_test), axis = 0)\n",
    "class_data = np.concatenate((y_train, y_test), axis = 0)\n",
    "# recuperando forma de imagenes\n",
    "img_shape = get_shape(img_data)\n",
    "# recuperando etiquetas de las clases\n",
    "labels = class2label(class_data, label_names)\n",
    "# estandarizar los datos de la imagen\n",
    "std_img_data = std_data(img_data, 0, 255)\n",
    "# categorizando las clases a aprender\n",
    "cat_labels = to_categorical(class_data, len(label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambio de formato para utilizar el dataframe\n",
    "img_data = img_data.tolist()\n",
    "std_img_data = std_img_data.tolist()\n",
    "cat_labels = cat_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir arreglo basico de datos\n",
    "data_list = (img_data, img_shape, class_data, labels, std_img_data, cat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poblamdo las columnas del dataframe\n",
    "for col, data in zip(col_names, data_list):\n",
    "    fashion_df[col] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libero memoria\n",
    "gc.collect()"
   ]
  },
  {
   "source": [
    "## Preprocesar los Datos\n",
    "\n",
    "Los pasos de esta seccion son:\n",
    "\n",
    "1. Revisar que los datos esten bien.\n",
    "2. Elegir la caracteristicas o propiedades de aprendizaje.\n",
    "3. Elegir la variable objetivo del aprendizaje.\n",
    "4. Dividir la conjunto de datos entre las poblaciones de entrenamiento y pruebas.\n",
    "5. Formatear los datos de aprendizaje y objetivo acorde a la red neuronal."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cchequeo la distribucion de datos\n",
    "sns.set()\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.histplot(fashion_df[col_names[3]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seleccionando caracteristicas de aprendizaje y variables objetivo\n",
    "# recuperando la forma de las imagenes basado en el primer elemento de la lista\n",
    "\n",
    "# recuperando los valores y ajustando el tensor para la CNN\n",
    "A = fashion_df[col_names[4]]\n",
    "# recuperando los valores de la cateogoria\n",
    "b = fashion_df[col_names[5]].values\n",
    "\n",
    "# fortateo de datos numpy\n",
    "X = np.array([np.array(i, dtype=\"object\") for i in A], dtype=\"object\")\n",
    "y = np.array([np.array(j, dtype=\"object\") for j in b], dtype=\"object\")\n",
    "\n",
    "print(X.shape)\n",
    "# forma basica general de las imagenes\n",
    "imgsh = X[0].shape\n",
    "# ajuste de forma para el modelo CNN\n",
    "X = X.reshape(fashion_df.shape[0], imgsh[0], imgsh[1], 1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semilla para el random\n",
    "rseed = 42\n",
    "\n",
    "# en tamanho de la muestra para pruebas esta entre 0.2 y 0.3\n",
    "train_pop = 0.8\n",
    "test_pop = 1.0 - train_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribuir los datos entre entrenamiento vs. pruebas\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_pop, random_state = rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formateo para keras y tensorflow\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=\"float64\")\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=\"float64\")\n",
    "X_test = tf.convert_to_tensor(X_test, dtype=\"float64\")\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=\"float64\")"
   ]
  },
  {
   "source": [
    "## Definir Modelo CNN\n",
    "\n",
    "Los pasos de esta seccion son:\n",
    "\n",
    "1. Definir las variables topologicas de la red neuronal.\n",
    "2. Definir los parametros de optimizacion y aprendizaje del modelo.\n",
    "3. Definir la topologia (capas) del modelo.\n",
    "4. Definir las condiciones de entrenamiento para el modelo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defino parametros necesarios para el modelo Autoencoder\n",
    "# parametros para las capas\n",
    "mid_neurons = 16*16\n",
    "filters = 16\n",
    "outn = len(label_names)\n",
    "ksize = (3,3)\n",
    "psize = (2,2)\n",
    "act = \"relu\"\n",
    "out = \"sigmoid\"\n",
    "pad = \"same\"\n",
    "ldrop = 0.2\n",
    "\n",
    "# forma del kernel de entrada y de las capas intermedias\n",
    "inshape = X[0].shape\n",
    "\n",
    "# parametros de optimizacion del modelo\n",
    "l = \"categorical_crossentropy\"\n",
    "opti = \"adam\"\n",
    "met = [\"accuracy\"]\n",
    "\n",
    "# parametros de operacion/aprendizaje del modelo\n",
    "ver = 1\n",
    "epo = 50\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definicion de las capas para el Autoencoder\n",
    "layers = (\n",
    "    # capa de entrada\n",
    "    Input(shape = inshape, name = \"LayIn\"),\n",
    "    # capa convolucional intermedia con regularizacion\n",
    "    Conv2D(filters, ksize, activation = act, padding = pad, name = \"EnConv1\"),\n",
    "    MaxPooling2D(psize, padding = pad, name = \"EnPool1\"),\n",
    "    # BatchNormalization(name = \"EnNorm1\"),\n",
    "    Dropout(ldrop, name = \"EnDrop1\"),\n",
    "\n",
    "    # capa convolucional intermedia con regularizacion\n",
    "    Conv2D(int(filters)/2, ksize, activation=act, padding = pad, name = \"EnConv2\"),\n",
    "    MaxPooling2D(psize, padding = pad, name = \"EnPool2\"),\n",
    "    # BatchNormalization(name = \"EnNorm2\"),\n",
    "    # Dropout(ldrop, name = \"EnDrop2\"),\n",
    "    \n",
    "    # capa intermedia de 2D a 1D\n",
    "    Flatten(name = \"LayFlat\"),\n",
    "    # # # capa intermedia densamente poblada con regularizacion\n",
    "    Dense(mid_neurons, activation = act, name = \"DenseMid\"),\n",
    "    Dropout(ldrop, name = \"MidDrop\"),\n",
    "    # # # capa intermedia de 1D a 2D\n",
    "    \n",
    "    # capa densamente poblada para clasificar los datos \n",
    "    Dense(int(mid_neurons)/2, activation = act, name = \"DenseClass1\"),\n",
    "    Dropout(ldrop, name = \"ClsDrop1\"),\n",
    "\n",
    "    # capa densamente poblada para clasificar los datos \n",
    "    Dense(int(mid_neurons)/4, activation = act, name = \"DenseClass2\"),\n",
    "    Dropout(ldrop, name = \"ClsDrop2\"),\n",
    "\n",
    "    # capa de salida\n",
    "    Dense(outn, activation=out, name = \"LayOut\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definiendo el modelo CNN en Keras\n",
    "cnn_model = Sequential(layers)\n",
    "cnn_model.model_name = \"DCNN Classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilando las condiciones de optimizacion y ajuste del Modelo CNN\n",
    "cnn_model.compile(loss = l, optimizer = opti, metrics = met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resumen de la topologia del modelo\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condiciones de parada temprana\n",
    "cnn_earlystop_acc = EarlyStopping(monitor = \"val_accuracy\", min_delta = 0.001, patience = 7, verbose = ver, mode = \"max\", restore_best_weights = True)"
   ]
  },
  {
   "source": [
    "## Entrenar Modelo\n",
    "\n",
    "Los pasos de esta seccion son:\n",
    "\n",
    "1. Entrenar el modelo con el conjunto de entrenamineto."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajustando el modelo MLP Keras\n",
    "cnn_log = cnn_model.fit(\n",
    "    x = X_train,#np.array(X_trainB), \n",
    "    y = y_train,#to_categorical(np.array(y_trainB), categories), \n",
    "    batch_size = bs,\n",
    "    epochs = epo, \n",
    "    verbose = ver,\n",
    "    callbacks = [cnn_earlystop_acc],\n",
    "    workers = 8,\n",
    "    shuffle = False,\n",
    "    use_multiprocessing = True,\n",
    "    validation_data = (X_test, y_test)\n",
    ")"
   ]
  },
  {
   "source": [
    "## Probar Modelo\n",
    "\n",
    "Los pasos de esta seccion son:\n",
    "\n",
    "1. Probar el modelo con el conjunto de pruebas.\n",
    "2. Evaluar globalmente los resultados.\n",
    "3. Guardar el modelo entrenado."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_eval = cnn_model.evaluate(x = X_test, y = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultados generales\n",
    "print(\"Perdida promedio: \", cnn_eval[0])\n",
    "print(\"Precision promedio: \", cnn_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruebas sobre el modelo\n",
    "cnn_predictions = cnn_model.predict(X_test, verbose = ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar el modelo entrenado\n",
    "wdir = os.getcwd()\n",
    "folder_models = \"Models\"\n",
    "model_fname = \"hdig_cnn_classifier\"\n",
    "model_fpn = os.path.join(folder_models, model_fname)\n",
    "print(\"El modelo entrenado esta en:\", model_fpn)\n",
    "cnn_model.save(model_fpn)\n",
    "# tf.keras.models.save_model(cnn_autoencoder, model_fpn)"
   ]
  },
  {
   "source": [
    "## Mostrar Resultados\n",
    "\n",
    "Los pasos de esta sección son:\n",
    "\n",
    "1. Mostrar las curvas de aprendizaje.\n",
    "2. Mostrar la Matriz de confusión del Clasificador.\n",
    "3. Mostrar la clasificación del modelo.\n",
    "4. Mostrar la abstracción del Clasificador."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajuste de las predicciones para ver el reporte de matrix de confusion\n",
    "cnn_predictions = np.array(cnn_predictions).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informe por consola de las pruebas para el clasificador\n",
    "print(\"----- Reporte de Pruebas para el clasificador CNN -----\")\n",
    "print(\"--- Conteo ---\\n\" + str(Counter(cnn_predictions)))\n",
    "print(\"--- Matriz de Confusion ---\\n\" + str(confusion_matrix(y_test, cnn_predictions)))\n",
    "print(\"--- Reporte de Pruebas: ---\")\n",
    "print(classification_report(y_test, cnn_predictions))\n",
    "print(\"--- Puntaje General---\\n\")\n",
    "print(\" - Perdida: \", cnn_eval[0])\n",
    "print(\" - Precision: \", cnn_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reporte del aprendizaje\n",
    "# base de la figura\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (16,8))\n",
    "\n",
    "# datos de la figura en de perdida y precision\n",
    "ax1.plot(cnn_log.history[\"loss\"], 'red', label = \"Train Loss\")\n",
    "ax1.plot(cnn_log.history[\"val_loss\"], 'darkorange', label = \"Test Loss\")\n",
    "ax2.plot(cnn_log.history[\"accuracy\"], 'red', label = \"Train Accuracy\")\n",
    "ax2.plot(cnn_log.history[\"val_accuracy\"], 'royalblue', label = \"Test Accuracy\")\n",
    "\n",
    "# leyenda de la grafica\n",
    "fig.suptitle(\"LEARNING BEHAVIOR\")\n",
    "ax1.grid(True)\n",
    "ax2.grid(True)\n",
    "ax1.set_title(\"Loss\")\n",
    "ax2.set_title(\"Accuracy\")\n",
    "ax1.set(xlabel = \"Epoch [cycle]\", ylabel = \"loss [%]\")\n",
    "ax2.set(xlabel = \"Epoch [cycle]\", ylabel = \"Acc [%]\")\n",
    "fig.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prueba funcional del autoencoder\n",
    "max_img = 10\n",
    "random_test_img = np.random.randint(len(X_test), size = max_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_layer = \"LayFlat\"\n",
    "cnn_abstraction = Model(inputs = cnn_autoencoder.input, outputs = cnn_autoencoder.get_layer(middle_layer).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desplegando pruebas\n",
    "plt.figure(figsize=(20, 4))\n",
    "og_shape = fashion_df[col_names[1]][0]\n",
    "\n",
    "for i, img_id in enumerate(random_test_img):\n",
    "    # imagen original\n",
    "    ax = plt.subplot(3, max_img, i + 1)\n",
    "    temp_X = np.array(X_test[img_id])\n",
    "    plt.imshow(temp_X.reshape(og_shape))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # imagen abstracta\n",
    "    ax = plt.subplot(3, max_img, max_img + i + 1)\n",
    "    temp_abstract = cnn_abstraction(temp_X)\n",
    "    temp_abstract = np.array(temp_abstract)\n",
    "    plt.imshow(temp_abstract)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # imagen reconstruida\n",
    "    ax = plt.subplot(3, max_img, 2*max_img + i + 1)\n",
    "    temp_pre = np.array(cnn_predictions[img_id])\n",
    "    plt.imshow(temp_pre.reshape(og_shape))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python386jvsc74a57bd01baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253",
   "display_name": "Python 3.8.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}